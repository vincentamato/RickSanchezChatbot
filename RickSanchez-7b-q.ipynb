{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d85f54-fa42-4d31-9e32-1acf0fc7dd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install accelerate peft bitsandbytes transformers trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6251ce-0ced-4deb-a7e6-26ff0c8fca53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from typing import List, Dict\n",
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "from peft import LoraConfig\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667dc528-2296-4a25-a8ed-d198a8d993ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "new_model = \"RickSanchez-7b-q\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57589b2-467a-4de6-8a80-50fa6cd9fb54",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Message:\n",
    "    \"\"\"\n",
    "    A class to represent a message in a dialogue.\n",
    "\n",
    "    Attributes:\n",
    "    role (str): The role of the speaker in the dialogue (e.g., 'user', 'assistant').\n",
    "    content (str): The actual text content of the message.\n",
    "    \"\"\"\n",
    "    def __init__(self, role: str, content: str):\n",
    "        \"\"\"\n",
    "        Initializes a new instance of the Message class.\n",
    "\n",
    "        Parameters:\n",
    "        role (str): The role of the speaker in the dialogue.\n",
    "        content (str): The text content of the message.\n",
    "        \"\"\"\n",
    "        self.role = role  # Assign the role to the instance variable\n",
    "        self.content = content  # Assign the content to the instance variable\n",
    "\n",
    "    def to_dict(self) -> Dict:\n",
    "        \"\"\"\n",
    "        Converts the Message instance into a dictionary format.\n",
    "\n",
    "        Returns:\n",
    "        Dict: A dictionary with 'role' and 'content' as keys.\n",
    "        \"\"\"\n",
    "        # Return the message as a dictionary with 'role' and 'content' keys\n",
    "        return {\"role\": self.role, \"content\": self.content}\n",
    "\n",
    "\n",
    "def format_dialogue_for_chat(line1: str, line2: str) -> List[Message]:\n",
    "    \"\"\"\n",
    "    Formats a pair of lines as a dialogue for chat completion.\n",
    "\n",
    "    This function takes two lines of dialogue, assigns roles to them ('user' and 'assistant'),\n",
    "    and creates Message objects for each line. It's designed to format dialogues where\n",
    "    the first line is from a user and the second line is a system (like Rick's) response.\n",
    "\n",
    "    Parameters:\n",
    "    line1 (str): The text of the first line in the dialogue.\n",
    "    line2 (str): The text of the second line in the dialogue.\n",
    "\n",
    "    Returns:\n",
    "    List[Message]: A list of two Message objects representing the dialogue.\n",
    "    \"\"\"\n",
    "    # Create a Message object for the user's line\n",
    "    user_message = Message(\"user\", line1)\n",
    "    # Create a Message object for the assistant's (Rick's) response\n",
    "    assistant_message = Message(\"assistant\", line2)\n",
    "\n",
    "    # Return the two messages as a list\n",
    "    return [user_message, assistant_message]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce62b89-c815-4d9e-b207-fb1847d19931",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_csv(csv_file_path: str) -> List[List[Message]]:\n",
    "    \"\"\"\n",
    "    Process a CSV file to extract dialogues.\n",
    "\n",
    "    This function reads a CSV file containing script lines, and extracts dialogues where one\n",
    "    of the lines is spoken by Rick. It assumes a specific format of the CSV where the speaker's\n",
    "    name is in the fifth column and the dialogue text is in the sixth column.\n",
    "\n",
    "    Parameters:\n",
    "    csv_file_path (str): The file path to the CSV file containing the script data.\n",
    "\n",
    "    Returns:\n",
    "    List[List[Message]]: A list of dialogues, each dialogue is a list containing two Message objects.\n",
    "                         The first Message is a line by any character (not Rick), and the second Message\n",
    "                         is a response by Rick.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize an empty list to store dialogues\n",
    "    dialogs = []\n",
    "\n",
    "    # Open the CSV file for reading\n",
    "    with open(csv_file_path, mode='r', encoding='utf-8') as file:\n",
    "        reader = csv.reader(file)\n",
    "\n",
    "        # Skip the header row\n",
    "        next(reader)\n",
    "\n",
    "        # Initialize a variable to keep track of the previous line\n",
    "        prev_line = None\n",
    "\n",
    "        # Iterate over each row in the CSV\n",
    "        for row in reader:\n",
    "            # If it's the first iteration, set prev_line and skip to the next row\n",
    "            if prev_line is None:\n",
    "                prev_line = row\n",
    "                continue\n",
    "\n",
    "            # Check if the current line is spoken by Rick\n",
    "            if row[4] == \"Rick\":\n",
    "                # If it is, format the previous line and the current line as a dialogue\n",
    "                dialogs.append(format_dialogue_for_chat(prev_line[5], row[5]))\n",
    "\n",
    "            # Update prev_line with the current row for the next iteration\n",
    "            prev_line = row\n",
    "\n",
    "    # Return the list of extracted dialogues\n",
    "    return dialogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50e07fb-dfdd-4804-ac0c-4ed0831f401f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the CSV file containing the Rick and Morty script data\n",
    "csv_file_path = 'RickAndMortyScripts.csv'\n",
    "\n",
    "# Call the function process_csv with the CSV file path. This function will read the CSV file,\n",
    "# extract dialogues involving Rick, and return them as a list of Message objects.\n",
    "dialogs = process_csv(csv_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c80343-25af-4edf-bae9-532bb7765f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_hf_chat_format(dialogs):\n",
    "    \"\"\"\n",
    "    Converts a list of dialogues into a format compatible with Hugging Face's chat models.\n",
    "\n",
    "    This function iterates through each dialogue in the provided list. Each dialogue is composed of\n",
    "    message objects which are formatted into a string that follows the conventions used by Hugging Face\n",
    "    chat models. Specifically, it adds special tokens to denote the start (BOS) and end (EOS) of each message,\n",
    "    as well as markers to indicate the role of the speaker (user or system).\n",
    "\n",
    "    Parameters:\n",
    "    dialogs (List[List[Message]]): A list of dialogues, where each dialogue is a list of Message objects.\n",
    "\n",
    "    Returns:\n",
    "    List[str]: A list of dialogues formatted as strings, suitable for Hugging Face chat models.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize an empty list to store the formatted dialogues\n",
    "    formatted_dialogs = []\n",
    "\n",
    "    # Iterate over each dialogue in the provided list\n",
    "    for dialog in dialogs:\n",
    "        # Initialize an empty string to build the chat input\n",
    "        chat_input = ''\n",
    "\n",
    "        # Iterate over each message in the dialogue\n",
    "        for message in dialog:\n",
    "            # Format messages from the 'user' role\n",
    "            if message.role == 'user':\n",
    "                # Add special tokens and the user's message content\n",
    "                chat_input += f'INST <<USER>> BOS {message.content.strip()} EOS '\n",
    "\n",
    "            # Format messages from the 'assistant' role\n",
    "            elif message.role == 'assistant':\n",
    "                # Add special tokens and the assistant's message content\n",
    "                chat_input += f'<<SYS>> BOS {message.content.strip()} EOS '\n",
    "\n",
    "        # Append the formatted chat input to the list, stripping any trailing spaces\n",
    "        formatted_dialogs.append(chat_input.strip())\n",
    "\n",
    "    # Return the list of formatted dialogues\n",
    "    return formatted_dialogs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fba48c-c49a-463b-8492-b30a05b7e31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the dialogues to a format compatible with Hugging Face's chat models.\n",
    "hf_formatted_dialogs = convert_to_hf_chat_format(dialogs)\n",
    "\n",
    "# Create a Hugging Face Dataset from the formatted dialogues.\n",
    "dataset = Dataset.from_dict({\"dialog\": hf_formatted_dialogs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd1090d-8854-4108-9429-990f27b3d2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the data type 'float16' from the torch module. This data type is used for half-precision floating-point numbers,\n",
    "# which can reduce memory usage and potentially increase performance during model computation, especially on GPUs.\n",
    "compute_dtype = getattr(torch, \"float16\")\n",
    "\n",
    "# Configure quantization settings for the BitsAndBytes library. This library optimizes model parameter storage and computation.\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,  # Enable loading model parameters in 4-bit precision to reduce memory usage.\n",
    "    bnb_4bit_use_double_quant=True,  # Use double quantization for 4-bit precision, enhancing the balance between memory savings and model fidelity.\n",
    "    bnb_4bit_quant_type=\"nf4\",  # Set the quantization type to 'nf4', a specific 4-bit quantization scheme.\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,  # Use bfloat16 (Brain Floating Point) for computation, offering a balance between precision and performance.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cc01c6-4b5c-4fca-b5fb-69cbb82c3c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pre-trained causal language model using the Hugging Face Transformers library.\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,  # 'base_model' is a variable holding the name or path of the pre-trained model.\n",
    "    quantization_config=quant_config,  # Apply the previously defined BitsAndBytes quantization configuration to optimize memory usage.\n",
    "    device_map={\"\": 0}  # Assign the model to the first available GPU (device 0). If running on CPU, this would be an empty string.\n",
    ")\n",
    "\n",
    "# Update the model's configuration.\n",
    "model.config.use_cache = False  # Disable caching of past hidden states. This can save memory in trade-off for speed, especially in generation tasks.\n",
    "model.config.pretraining_tp = 1  # Set 'pretraining_tp' (tensor parallelism during pretraining) to 1, indicating no parallelism is used. This is relevant for multi-GPU setups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08381358-a61f-4f7b-9e10-400fe513c813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer for the specified base model with the ability to include custom tokenization logic.\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)\n",
    "\n",
    "# Set the tokenizer's padding token to be the same as its end-of-sequence (EOS) token.\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Configure the tokenizer to add padding (if necessary) to the right side of the sequence.\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f73d50-9cf5-4133-b4e6-21465ff999a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for Low-Rank Adaptation (LoRA)\n",
    "peft_params = LoraConfig(\n",
    "    r=8,  # The rank for the low-rank matrices in LoRA, affecting the number of parameters to be trained.\n",
    "    lora_alpha=32,  # A scaling factor for the low-rank matrices, controlling the magnitude of updates.\n",
    "    lora_dropout=0.05,  # Dropout probability applied to LoRA layers to prevent overfitting.\n",
    "    bias=\"none\",  # Specifies whether to apply bias in the LoRA layers; 'none' indicates no bias.\n",
    "    task_type=\"CAUSAL_LM\",  # The type of task for the language model, here set for causal language modeling.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57360355-c724-40ac-a46b-c468b005e94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for training parameters using the Hugging Face Transformers library.\n",
    "training_params = TrainingArguments(\n",
    "    output_dir=\"./results\",  # Directory where the training results and model checkpoints will be saved.\n",
    "    max_steps=150,  # Maximum number of training steps to perform.\n",
    "    save_steps=50,  # Save a model checkpoint after this many steps.\n",
    "    gradient_accumulation_steps=2,  # Number of steps to accumulate gradients before performing a backward/update pass.\n",
    "    learning_rate=2e-4,  # Learning rate for the optimizer.\n",
    "    per_device_train_batch_size=4,  # Batch size per device during training.\n",
    "    warmup_steps=2,  # Number of warmup steps for learning rate scheduler.\n",
    "    logging_steps=1,  # Log training information every this many steps.\n",
    "    fp16=True,  # Whether to use 16-bit (mixed) precision instead of 32-bit.\n",
    "    seed=42,  # Random seed for initialization, ensuring reproducibility.\n",
    "    optim=\"paged_adamw_8bit\",  # The optimizer to use. Here, it's a specific 8-bit version of AdamW for efficiency.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a56376-08db-46d1-a667-972b3e9425a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization of the SFTTrainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,  # The pre-trained language model to be fine-tuned.\n",
    "    train_dataset=dataset,  # The dataset to be used for training.\n",
    "    peft_config=peft_params,  # The Low-Rank Adaptation (LoRA) configuration for efficient fine-tuning.\n",
    "    dataset_text_field=\"dialog\",  # The field in the dataset that contains the input text for training.\n",
    "    max_seq_length=None,  # Maximum sequence length for model inputs. 'None' means it will use the model's default.\n",
    "    tokenizer=tokenizer,  # Tokenizer to be used for converting text into model input format.\n",
    "    args=training_params,  # Training arguments such as learning rate, batch size, etc.\n",
    "    packing=False,  # Determines whether to use data packing. 'False' means no packing will be used.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6219228-3d3b-49b3-b54e-fb6283cd1ea1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train the model.\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff347f6a-243d-4d35-bf86-db91cfcf925d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model to the specified directory.\n",
    "trainer.model.save_pretrained('RickSanchez-7b-q')\n",
    "\n",
    "# Save the tokenizer associated with the model to the same directory.\n",
    "trainer.tokenizer.save_pretrained('RickSanchez-7b-q')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63fd00c-57d5-48f2-8269-c2739b6f5f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the fine-tuned model from the specified directory.\n",
    "model = AutoModelForCausalLM.from_pretrained('RickSanchez-7b-q')\n",
    "\n",
    "# Load the tokenizer that was used during the training of this model.\n",
    "tokenizer = AutoTokenizer.from_pretrained('RickSanchez-7b-q')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598678c0-0e02-43d3-bcbf-05af4a8cf95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing the prompt for generating a response using the fine-tuned language model.\n",
    "\n",
    "# Define the system prompt. This sets the context for the language model, instructing it to respond as Rick Sanchez.\n",
    "system_prompt = \"You are Rick Sanchez from Rick and Morty. You are an eccentric and cynical genius. Respond in complete sentences, and only as Rick.\"\n",
    "\n",
    "# User input to the model. This represents a question or statement that Rick Sanchez is expected to respond to.\n",
    "# In this case, the user (Morty) is asking if they can go on an adventure.\n",
    "user_input = \"Can we go on an adventure Rick?\"\n",
    "\n",
    "# Combining the system prompt with the user input to form the full prompt for the model.\n",
    "# This format sets the scene for the interaction, with Morty's line followed by an expectation for Rick's response.\n",
    "full_prompt = f\"{system_prompt}\\nMorty: {user_input}\\nRick:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b56f81b-0a36-485a-a552-62ab23fbd0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(full_prompt, max_length=100, num_beams=3):\n",
    "    \"\"\"\n",
    "    Generate a response from the model based on a given prompt using beam search.\n",
    "\n",
    "    This function takes a prompt, encodes it for the model, and generates a response. \n",
    "    It uses beam search to enhance the quality of the response. The function is optimized for \n",
    "    memory usage by clearing the CUDA cache and using torch.no_grad().\n",
    "\n",
    "    Args:\n",
    "        full_prompt (str): The prompt to which the model will generate a response.\n",
    "        max_length (int): The maximum length of the model's response.\n",
    "        num_beams (int): The number of beams to use in beam search for diversity in responses.\n",
    "\n",
    "    Returns:\n",
    "        str: The text generated by the model as a response to the prompt.\n",
    "\n",
    "    The generated text is post-processed to ensure proper formatting, remove any 'EOS' tokens,\n",
    "    and to ensure it doesn't contain the initial prompt.\n",
    "    \"\"\"\n",
    "    # Set the device for computation based on CUDA availability\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model.to(device)\n",
    "\n",
    "    # Encode the combined prompt\n",
    "    input_ids = tokenizer.encode(full_prompt, return_tensors='pt').to(device)\n",
    "\n",
    "    # Generate response using the model with beam search\n",
    "    output = model.generate(\n",
    "        input_ids, \n",
    "        max_length=max_length,\n",
    "        num_beams=num_beams,  # Set number of beams for beam search\n",
    "        no_repeat_ngram_size=2,  # Prevents the model from repeating short phrases\n",
    "        early_stopping=True,    # Stops when a sentence is completed\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    # Decode the output\n",
    "    response_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "    # Truncate at the last complete sentence if possible\n",
    "    sentences = response_text.split('.')\n",
    "    response_text = '.'.join(sentences[:-1]) + ('.' if len(sentences) > 1 else '')\n",
    "\n",
    "    # Remove the 'EOS' token and any trailing spaces from the response\n",
    "    response_text = response_text.replace(\"EOS\", \"\").strip()\n",
    "\n",
    "    # Ensure the response does not contain the full prompt\n",
    "    if response_text.startswith(full_prompt):\n",
    "        response_text = response_text[len(full_prompt):]\n",
    "\n",
    "    # Trim leading and trailing spaces again after adjustments\n",
    "    response_text = response_text.strip()\n",
    "\n",
    "    return response_text\n",
    "\n",
    "# Example usage\n",
    "print(generate_response(full_prompt))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RickBot-7B",
   "language": "python",
   "name": "rickbot-7b"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
